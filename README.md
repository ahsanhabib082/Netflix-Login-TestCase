# Manual Testing (Netflix-Login-TestCase)
This repository contains test artifacts for the Manual Testing of the Netflix Login.Netflix is a popular subscription-based streaming service that provides access to a vast library of TV shows, movies, documentaries, and original content.The focus of this testing effort has been on the sign up and sign in modules. The repository includes  test execution reports summary, test cases activities for Netflix.

# Table of Contents

• Introduction

• Project Overview

• Test Artifacts

• Contributing

• FAQ


# Introduction
Manual testing plays a vital role in ensuring the quality of a system. This README provides a comprehensive overview of the manual test cases designed to evaluate the functionality and reliability of the Netflix login process. The test cases cover a range of scenarios, including successful logins, failed login attempts, and edge cases, to ensure a robust and user-friendly login experience.

# Project Overview
This README provides a detailed overview of the manual test cases designed to comprehensively evaluate the Netflix login functionality. The primary objective of these tests is to ensure the reliability, security, and user-friendliness of the login process. By executing a diverse range of test scenarios, including successful logins, failed login attempts, edge cases, and security checks, we aim to identify and rectify any potential issues that may hinder a seamless user experience. These test cases will help guarantee that users can effortlessly access their Netflix accounts and enjoy a smooth streaming experience.

# Test Artifacts
The following test artifacts are included in this project:

• Mindmap ( will add later)

• Test Execution Report

• Test Cases

• Test Metrics( will add later )


# Contributing

Contributions are welcome to enhance the repository with additional test artifacts, such as test cases, mindmaps, test execution reports, or test metrics, related to other modules of the Netflix log-in test cases. If you have any improvements or suggestions, please feel free to open an issue or submit a pull request. Please follow the repository's code of conduct and licensing terms.

# FAQ
Q1: How can I contribute to this project?

Answer: Contributions are welcome! You can contribute by adding new test cases, scenarios, mindmaps, reports, or test metrics to the respective directories. Simply open an issue or submit a pull request with your proposed changes.

Q2: Can I use the test cases and scenarios provided in this repository for my own projects?

Answer: Absolutely! The test cases and scenarios in this repository are meant to be shared and used as a reference for manual testing projects. Feel free to adapt and utilize them for your own testing efforts.

Q3: Is there a specific format or template to follow for test cases?

Answer: While there isn't a strict template to follow, we recommend including the test case ID, description, pre-requisites, test steps, and expected results. You can also add any additional fields that are relevant to your project. Consistency in format across test cases will make it easier to navigate and understand the test suite.

Q4: Are all Test processes integrated in one file?

Answer: Yes. In the excel file you can see test cases, scenarios, mindmaps, test report and test metrics. All in one. The focus of this testing effort has been on the sign up and sign in modules. 




